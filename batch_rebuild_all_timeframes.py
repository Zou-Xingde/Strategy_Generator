#!/usr/bin/env python3
"""
åˆ†æ‰¹é‡æ–°è™•ç†æ‰€æœ‰æ™‚é–“æ¡†æ¶çš„è Ÿç‡­åœ–è³‡æ–™
åŒ…å«å®Œæ•´çš„è³‡æ–™é©—è­‰å’Œæª¢æŸ¥
"""

import duckdb
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import shutil
from pathlib import Path
from typing import Dict, List, Tuple
import time

# è¨­å®š
DUCKDB_PATH = "database/market_data.duckdb"
CHUNK_ROWS = 100_000  # æ¯æ¬¡è™•ç†çš„è³‡æ–™ç­†æ•¸
MAX_RECORDS_PER_INSERT = 5_000  # æ¯æ¬¡æ’å…¥çš„æœ€å¤§è¨˜éŒ„æ•¸

# å•†å“é…ç½®
SYMBOLS_CONFIG = {
    "XAUUSD": {
        "name": "é»ƒé‡‘",
        "tick_file": r"D:\project\ç­–ç•¥ç”¢ç”Ÿå™¨\data\Strategy_GeneratorXAUUSD_dukascopy_TICK_UTC-TICK-No Session.csv"
    },
    "US30": {
        "name": "é“ç“Šæ–¯",
        "tick_file": r"D:\project\ç­–ç•¥ç”¢ç”Ÿå™¨\data\Strategy_GeneratorUSA30IDXUSD_dukascopy_TICK_UTC-TICK-No Session.csv"
    },
    "US100": {
        "name": "ç´æ–¯é”å…‹",
        "tick_file": r"D:\project\ç­–ç•¥ç”¢ç”Ÿå™¨\data\Strategy_GeneratorUSATECHIDXUSD_dukascopy_TICK_UTC-TICK-No Session.csv"
    }
}

# æ™‚é–“æ¡†æ¶é…ç½®
TIMEFRAMES = {
    "M1": "1min",
    "M5": "5min", 
    "M15": "15min",
    "M30": "30min",
    "H1": "1h",
    "H4": "4h",
    "D1": "1D",
    "W1": "1W",
    "MN": "1M"
}

def clear_screen():
    """æ¸…å±"""
    os.system('cls' if os.name == 'nt' else 'clear')

def backup_database():
    """å‚™ä»½è³‡æ–™åº«"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = f"database/backup_before_rebuild_{timestamp}.duckdb"
    
    if os.path.exists(DUCKDB_PATH):
        print(f"ğŸ”„ å‚™ä»½è³‡æ–™åº«åˆ°: {backup_path}")
        shutil.copy2(DUCKDB_PATH, backup_path)
        print("âœ… è³‡æ–™åº«å‚™ä»½å®Œæˆ")
    else:
        print("âš ï¸ è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨")
    
    return backup_path

def check_file_exists(file_path: str) -> bool:
    """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(file_path):
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        return False
    
    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB
    print(f"âœ… æª”æ¡ˆå­˜åœ¨: {file_path} ({file_size:.1f} MB)")
    return True

def verify_csv_structure(file_path: str, symbol: str) -> bool:
    """é©—è­‰ CSV æª”æ¡ˆçµæ§‹"""
    try:
        print(f"ğŸ” é©—è­‰ {symbol} CSV çµæ§‹...")
        df_sample = pd.read_csv(file_path, nrows=10)
        
        print(f"ğŸ“Š æ¬„ä½: {list(df_sample.columns)}")
        print(f"ğŸ“‹ å‰3ç­†è³‡æ–™:")
        print(df_sample.head(3).to_string(index=False))
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_columns = ["DateTime", "Bid", "Ask", "Volume"]
        missing_columns = [col for col in required_columns if col not in df_sample.columns]
        
        if missing_columns:
            print(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
            return False
        
        print("âœ… CSV çµæ§‹é©—è­‰é€šé")
        return True
        
    except Exception as e:
        print(f"âŒ CSV çµæ§‹é©—è­‰å¤±æ•—: {e}")
        return False

def setup_database_schema(conn):
    """è¨­å®šè³‡æ–™åº«çµæ§‹"""
    print("ğŸ”§ è¨­å®šè³‡æ–™åº«çµæ§‹...")
    
    # æ¸…ç©ºèˆŠè³‡æ–™ï¼ˆä¿ç•™çµæ§‹ï¼‰
    try:
        conn.execute("DELETE FROM candlestick_data_new")
        print("ğŸ—‘ï¸ æ¸…ç©ºèˆŠè Ÿç‡­åœ–è³‡æ–™")
    except:
        pass
    
    # ç¢ºä¿ symbols è¡¨æœ‰æ­£ç¢ºçš„è³‡æ–™
    conn.execute("DELETE FROM symbols")
    for symbol in SYMBOLS_CONFIG.keys():
        conn.execute("INSERT INTO symbols (symbol) VALUES (?)", [symbol])
    
    # ç¢ºä¿ timeframes è¡¨æœ‰æ­£ç¢ºçš„è³‡æ–™
    conn.execute("DELETE FROM timeframes") 
    for tf_code, tf_seconds in [
        ("M1", 60), ("M5", 300), ("M15", 900), ("M30", 1800),
        ("H1", 3600), ("H4", 14400), ("D1", 86400), ("W1", 604800), ("MN", 2592000)
    ]:
        conn.execute("INSERT INTO timeframes (code, seconds) VALUES (?, ?)", [tf_code, tf_seconds])
    
    # é‡æ–°å‰µå»º candlestick_data_new è¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    conn.execute("""
        CREATE TABLE IF NOT EXISTS candlestick_data_new (
            id          BIGINT PRIMARY KEY,
            symbol      TEXT NOT NULL,
            timeframe   TEXT NOT NULL,
            timestamp   TIMESTAMP NOT NULL,
            open        DECIMAL(20,10) NOT NULL,
            high        DECIMAL(20,10) NOT NULL,
            low         DECIMAL(20,10) NOT NULL,
            close       DECIMAL(20,10) NOT NULL,
            volume      DECIMAL(38,18) NOT NULL,
            data_version TEXT NOT NULL,
            data_source  TEXT,
            created_at   TIMESTAMP DEFAULT now(),
            updated_at   TIMESTAMP DEFAULT now(),
            UNIQUE (symbol, timeframe, timestamp, data_version)
        )
    """)
    
    # é‡æ–°å‰µå»ºè¦–åœ–
    conn.execute("DROP VIEW IF EXISTS v_candlestick_latest")
    conn.execute("""
        CREATE VIEW v_candlestick_latest AS
        SELECT * EXCLUDE rn
        FROM (
            SELECT *,
                   ROW_NUMBER() OVER (
                     PARTITION BY symbol, timeframe, timestamp
                     ORDER BY data_version DESC, updated_at DESC
                   ) AS rn
            FROM candlestick_data_new
        )
        WHERE rn = 1
    """)
    
    conn.commit()
    print("âœ… è³‡æ–™åº«çµæ§‹è¨­å®šå®Œæˆ")

def process_tick_to_candlesticks(file_path: str, symbol: str, conn) -> Dict[str, int]:
    """è™•ç† Tick è³‡æ–™ç”Ÿæˆæ‰€æœ‰æ™‚é–“æ¡†æ¶çš„è Ÿç‡­åœ–"""
    print(f"\nğŸš€ é–‹å§‹è™•ç† {symbol} çš„ Tick è³‡æ–™...")
    
    total_processed = 0
    timeframe_counts = {tf: 0 for tf in TIMEFRAMES.keys()}
    
    try:
        # è®€å–æª”æ¡ˆç¸½è¡Œæ•¸
        with open(file_path, 'r', encoding='utf-8') as f:
            total_lines = sum(1 for _ in f) - 1  # æ¸›å»æ¨™é¡Œè¡Œ
        
        print(f"ğŸ“Š ç¸½è¨ˆ {total_lines:,} ç­† Tick è³‡æ–™")
        
        # åˆ†æ‰¹è™•ç†
        chunk_iter = pd.read_csv(file_path, chunksize=CHUNK_ROWS)
        chunk_num = 0
        
        for chunk in chunk_iter:
            chunk_num += 1
            chunk_start_time = time.time()
            
            print(f"\nğŸ“¦ è™•ç†ç¬¬ {chunk_num} æ‰¹æ¬¡: {len(chunk):,} ç­†")
            
            # è³‡æ–™æ¸…ç†å’Œè½‰æ›
            chunk = chunk.rename(columns={"DateTime": "timestamp"})
            chunk["timestamp"] = pd.to_datetime(chunk["timestamp"], errors='coerce')
            chunk = chunk.dropna(subset=["timestamp"])
            chunk = chunk.sort_values("timestamp")
            
            # è¨ˆç®— mid price ä½œç‚ºè Ÿç‡­åœ–è³‡æ–™
            chunk["price"] = (chunk["Bid"] + chunk["Ask"]) / 2
            chunk["volume"] = chunk.get("Volume", 1000000)  # é è¨­æˆäº¤é‡
            
            # è¨­å®šç´¢å¼•ç‚ºæ™‚é–“æˆ³
            chunk.set_index("timestamp", inplace=True)
            
            # ç‚ºæ¯å€‹æ™‚é–“æ¡†æ¶ç”Ÿæˆè Ÿç‡­åœ–è³‡æ–™
            for tf_code, tf_pandas in TIMEFRAMES.items():
                try:
                    # é‡æ–°å–æ¨£ç”Ÿæˆ OHLC è³‡æ–™
                    ohlc_data = chunk["price"].resample(tf_pandas).ohlc()
                    volume_data = chunk["volume"].resample(tf_pandas).sum()
                    
                    # åˆä½µè³‡æ–™
                    candlestick_df = pd.DataFrame({
                        "open": ohlc_data["open"],
                        "high": ohlc_data["high"], 
                        "low": ohlc_data["low"],
                        "close": ohlc_data["close"],
                        "volume": volume_data
                    })
                    
                    # ç§»é™¤ç„¡æ•ˆè³‡æ–™
                    candlestick_df = candlestick_df.dropna()
                    
                    if len(candlestick_df) > 0:
                        # æº–å‚™æ’å…¥è³‡æ–™
                        insert_data = []
                        current_time = datetime.now()
                        
                        for timestamp, row in candlestick_df.iterrows():
                            insert_data.append({
                                "symbol": symbol,
                                "timeframe": tf_code,
                                "timestamp": timestamp,
                                "open": round(float(row["open"]), 10),
                                "high": round(float(row["high"]), 10),
                                "low": round(float(row["low"]), 10),
                                "close": round(float(row["close"]), 10),
                                "volume": round(float(row["volume"]), 18),
                                "data_version": "v2.0",
                                "data_source": "batch_rebuild",
                                "created_at": current_time,
                                "updated_at": current_time
                            })
                        
                        # åˆ†æ‰¹æ’å…¥é¿å…è¨˜æ†¶é«”å•é¡Œ
                        for i in range(0, len(insert_data), MAX_RECORDS_PER_INSERT):
                            batch = insert_data[i:i + MAX_RECORDS_PER_INSERT]
                            
                            # ä½¿ç”¨ INSERT OR REPLACE è™•ç†é‡è¤‡è³‡æ–™
                            for record in batch:
                                conn.execute("""
                                    INSERT OR REPLACE INTO candlestick_data_new 
                                    (symbol, timeframe, timestamp, open, high, low, close, volume, 
                                     data_version, data_source, created_at, updated_at)
                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """, [
                                    record["symbol"], record["timeframe"], record["timestamp"],
                                    record["open"], record["high"], record["low"], record["close"],
                                    record["volume"], record["data_version"], record["data_source"],
                                    record["created_at"], record["updated_at"]
                                ])
                            
                            timeframe_counts[tf_code] += len(batch)
                        
                        print(f"   {tf_code}: +{len(insert_data):,} æ ¹")
                        
                except Exception as e:
                    print(f"   âŒ {tf_code} è™•ç†å¤±æ•—: {e}")
            
            total_processed += len(chunk)
            chunk_elapsed = time.time() - chunk_start_time
            
            # å®šæœŸæäº¤
            if chunk_num % 3 == 0:
                conn.commit()
                print(f"ğŸ’¾ å·²æäº¤ç¬¬ {chunk_num} æ‰¹æ¬¡")
            
            progress_pct = (total_processed / total_lines) * 100
            print(f"ğŸ“ˆ é€²åº¦: {total_processed:,}/{total_lines:,} ({progress_pct:.1f}%) - è€—æ™‚: {chunk_elapsed:.1f}s")
            
        # æœ€çµ‚æäº¤
        conn.commit()
        print(f"âœ… {symbol} è™•ç†å®Œæˆï¼")
        
        return timeframe_counts
        
    except Exception as e:
        print(f"âŒ è™•ç† {symbol} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return timeframe_counts

def verify_data_integrity(conn, symbol: str) -> bool:
    """é©—è­‰è³‡æ–™å®Œæ•´æ€§"""
    print(f"\nğŸ” é©—è­‰ {symbol} è³‡æ–™å®Œæ•´æ€§...")
    
    try:
        # æª¢æŸ¥æ¯å€‹æ™‚é–“æ¡†æ¶çš„è³‡æ–™ç­†æ•¸
        for tf_code in TIMEFRAMES.keys():
            result = conn.execute("""
                SELECT COUNT(*) FROM candlestick_data_new 
                WHERE symbol = ? AND timeframe = ?
            """, [symbol, tf_code]).fetchone()
            
            count = result[0] if result else 0
            print(f"   {tf_code}: {count:,} æ ¹")
        
        # æª¢æŸ¥è³‡æ–™ç¯„åœ
        result = conn.execute("""
            SELECT MIN(timestamp) as start_time, MAX(timestamp) as end_time
            FROM candlestick_data_new 
            WHERE symbol = ?
        """, [symbol]).fetchone()
        
        if result and result[0]:
            print(f"   æ™‚é–“ç¯„åœ: {result[0]} åˆ° {result[1]}")
        
        # æª¢æŸ¥è³‡æ–™å“è³ª
        result = conn.execute("""
            SELECT COUNT(*) FROM candlestick_data_new 
            WHERE symbol = ? AND (
                open IS NULL OR high IS NULL OR low IS NULL OR close IS NULL OR
                high < GREATEST(open, close) OR low > LEAST(open, close)
            )
        """, [symbol]).fetchone()
        
        invalid_count = result[0] if result else 0
        if invalid_count > 0:
            print(f"   âš ï¸ ç™¼ç¾ {invalid_count} ç­†ç„¡æ•ˆè³‡æ–™")
            return False
        else:
            print("   âœ… è³‡æ–™å“è³ªæª¢æŸ¥é€šé")
            return True
        
    except Exception as e:
        print(f"   âŒ è³‡æ–™å®Œæ•´æ€§é©—è­‰å¤±æ•—: {e}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    clear_screen()
    print("ğŸš€ åˆ†æ‰¹é‡å»ºæ‰€æœ‰æ™‚é–“æ¡†æ¶è Ÿç‡­åœ–è³‡æ–™")
    print("=" * 60)
    
    # æª¢æŸ¥æª”æ¡ˆ
    print("\nğŸ“ æª¢æŸ¥åŸå§‹ Tick æª”æ¡ˆ...")
    all_files_exist = True
    for symbol, config in SYMBOLS_CONFIG.items():
        print(f"\nğŸ”¸ {config['name']} ({symbol}):")
        file_exists = check_file_exists(config['tick_file'])
        if file_exists:
            csv_valid = verify_csv_structure(config['tick_file'], symbol)
            if not csv_valid:
                all_files_exist = False
        else:
            all_files_exist = False
    
    if not all_files_exist:
        print("\nâŒ éƒ¨åˆ†æª”æ¡ˆä¸å­˜åœ¨æˆ–æ ¼å¼éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘")
        return
    
    # å‚™ä»½è³‡æ–™åº«
    print(f"\nğŸ’¾ å‚™ä»½è³‡æ–™åº«...")
    backup_path = backup_database()
    
    # é€£æ¥è³‡æ–™åº«
    print(f"\nğŸ”Œ é€£æ¥è³‡æ–™åº«...")
    conn = duckdb.connect(DUCKDB_PATH)
    
    # è¨­å®šè³‡æ–™åº«çµæ§‹
    setup_database_schema(conn)
    
    # è™•ç†æ¯å€‹å•†å“
    total_start_time = time.time()
    all_results = {}
    
    for symbol, config in SYMBOLS_CONFIG.items():
        print(f"\n{'='*60}")
        print(f"ğŸ¯ è™•ç† {config['name']} ({symbol})")
        print(f"{'='*60}")
        
        symbol_start_time = time.time()
        
        # è™•ç† Tick è³‡æ–™ç”Ÿæˆè Ÿç‡­åœ–
        timeframe_counts = process_tick_to_candlesticks(
            config['tick_file'], symbol, conn
        )
        
        # é©—è­‰è³‡æ–™å®Œæ•´æ€§
        data_valid = verify_data_integrity(conn, symbol)
        
        symbol_elapsed = time.time() - symbol_start_time
        print(f"â±ï¸ {symbol} è™•ç†è€—æ™‚: {symbol_elapsed/60:.1f} åˆ†é˜")
        
        all_results[symbol] = {
            "timeframe_counts": timeframe_counts,
            "data_valid": data_valid,
            "elapsed_time": symbol_elapsed
        }
    
    # é—œé–‰è³‡æ–™åº«é€£æ¥
    conn.close()
    
    # é¡¯ç¤ºç¸½çµå ±å‘Š
    total_elapsed = time.time() - total_start_time
    print(f"\n{'='*60}")
    print("ğŸ“Š è™•ç†å®Œæˆç¸½çµ")
    print(f"{'='*60}")
    print(f"â±ï¸ ç¸½è€—æ™‚: {total_elapsed/60:.1f} åˆ†é˜")
    print(f"ğŸ’¾ å‚™ä»½æª”æ¡ˆ: {backup_path}")
    
    print(f"\nğŸ“ˆ å„å•†å“è™•ç†çµæœ:")
    for symbol, results in all_results.items():
        config = SYMBOLS_CONFIG[symbol]
        print(f"\nğŸ”¸ {config['name']} ({symbol}):")
        print(f"   ç‹€æ…‹: {'âœ… æˆåŠŸ' if results['data_valid'] else 'âŒ å¤±æ•—'}")
        print(f"   è€—æ™‚: {results['elapsed_time']/60:.1f} åˆ†é˜")
        
        for tf_code, count in results['timeframe_counts'].items():
            if count > 0:
                print(f"   {tf_code}: {count:,} æ ¹")
    
    print(f"\nğŸ‰ æ‰€æœ‰å•†å“è™•ç†å®Œæˆï¼")
    print(f"ğŸ’¡ æ‚¨ç¾åœ¨å¯ä»¥åœ¨å‰ç«¯æŸ¥çœ‹æ‰€æœ‰æ™‚é–“æ¡†æ¶çš„è Ÿç‡­åœ–è³‡æ–™")

if __name__ == "__main__":
    main()